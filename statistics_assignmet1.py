# -*- coding: utf-8 -*-
"""Statistics_Assignmet1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/0x1ana/Case-study-of-Decision-Tree-Classification-and-Random-Forest-Classification/blob/main/Statistics_Assignmet1.ipynb
"""

!pip install --upgrade tensorflow

from __future__ import print_function
import pandas as pd
pd.__version__

import os
os.getcwd()

from google.colab import drive
drive.mount('/content/gdrive')

#Loading a dataset into a dataframe
#Use describe(), info(), head() functions to get simple statistics and a description of the dataset
from google.colab.data_table import DataTable
DataTable.max_columns = 58
spam_dataset_dataframe = pd.read_csv("/content/gdrive/My Drive/spam.csv", sep = ',')
print('Dataset Loaded...')
spam_dataset_dataframe.describe()
#Use spam
#spam_dataset_dataframe.iloc[0:100]

#Use spam
#spam_dataset_dataframe.iloc[0:3]
#spam_dataset_dataframe['Class']=spam_dataset_dataframe['Class'].apply(lambda x: 1 if x=='spam' else 0)
#Use iloc to acces each row, index starts at 0
spam_dataset_dataframe.iloc[0:10]

#lot per feature histogram. figsize = (width, height)
#spam_dataset_dataframe.hist(bins = 100,figsize=(15,10))

spam_dataset_dataframe['Class']=spam_dataset_dataframe['Class'].apply(lambda x: 1 if x=='spam' else 0)
spam_dataset_dataframe.head()

print(spam_dataset_dataframe)

# Visualising the Sparse Matrix

plt.figure(figsize=[15,30])
plt.title('')
plt.spy(spam_dataset_dataframe[:100].values, precision = 0.1, markersize = 5)
plt.show()

import numpy as np

#Define a function to create a training and test set.
#Takes dataframe and split ratio as input and outputs train and test datasets (dataframes)
def split_train_test(data,test_ratio):
  np.random.seed(42) # fix the seed if you want to generate the same
  shuffled_indices = np.random.permutation(len(data))
  test_set_size = int(len(data)*test_ratio)
  test_indices = shuffled_indices[:test_set_size]
  train_indices = shuffled_indices[test_set_size:]
  return data.iloc[train_indices], data.iloc[test_indices]

from sklearn.model_selection import train_test_split

#You can also use Scikit-Learn to create a training and test set.
#random_state parameter fixes the seed.
spam_training_set, spam_test_set = train_test_split(spam_dataset_dataframe, test_size=3601,random_state=42)
spam_dataset_dataframe.keys()
#spam_test_set.head()

spam_training_data, spam_training_target = spam_training_set[["make", "address", "all","3d","our", "over", "remove", "internet", "order", "mail", "receive", "will", "people", "report", "addresses", "free", "business", "email", "you", "credit", "your", "font", "0", "money", "hp", "hpl", "george", "650", "lab", "labs", "telnet", "857", "data", "415", "85", "technology", "1999", "parts", "pm", "direct", "cs", "meeting", "original", "project", "re", "edu", "table", "conference", "semicol", "paren", "bracket", "bang", "dollar", "pound", "cap_avg", "cap_long", "cap_total" ]], spam_training_set["Class"]
spam_test_data, spam_test_target = spam_test_set[["make", "address", "all","3d","our", "over", "remove", "internet", "order", "mail", "receive", "will", "people", "report", "addresses", "free", "business", "email", "you", "credit", "your", "font", "0", "money", "hp", "hpl", "george", "650", "lab", "labs", "telnet", "857", "data", "415", "85", "technology", "1999", "parts", "pm", "direct", "cs", "meeting", "original", "project", "re", "edu", "table", "conference", "semicol", "paren", "bracket", "bang", "dollar", "pound", "cap_avg", "cap_long", "cap_total"]], spam_test_set["Class"]

spam_training_data.head()

# Commented out IPython magic to ensure Python compatibility.
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
# data visualization
import seaborn as sns
# %matplotlib inline
from matplotlib import pyplot as plt
from matplotlib import style
from sklearn.metrics import confusion_matrix

criterion_DT = ["gini", "log_loss", "entropy"]
splitter_DT = ["best", "random"]
max_features_DT = ["sqrt", "log2"]
highest_accuracy_DT = 0
f = ""
g = ""
h = ""

for x in max_features_DT:
  for n in criterion_DT:
    for z in splitter_DT:
      clf = DecisionTreeClassifier(criterion = n, max_features = x, splitter= z, random_state = 101)
      clf.fit(spam_training_data,spam_training_target)
      spam_test_target_predict=clf.predict(spam_test_data)
      print("For criterion_DT = ",n,", and max_features_DT = ", x , "and splitter_DT = ", z, "the accuracy score is: ", accuracy_score(spam_test_target,spam_test_target_predict))

      if accuracy_score(spam_test_target,spam_test_target_predict) > highest_accuracy_DT:
        highest_accuracy_DT = accuracy_score(spam_test_target,spam_test_target_predict)
        f = str(n)
        g = str(x)
        h = str(z)

print("The decision tree with the highest accuracy ",highest_accuracy_DT, "has the following parameters: criterion_DT = ", f, " max_features_DT = ", g, "splitter_DT = ", h)

#clf = RandomForestClassifier(n_estimators=10, max_features="sqrt", random_state = 101)
#clf = DecisionTreeClassifier(criterion = "entropy", random_state = 101, max_features = )
clf1 = DecisionTreeClassifier(criterion = f, max_features = g, splitter = h, random_state = 101)

#clf = KNeighborsClassifier(n_neighbors=3)
#clf = AdaBoostClassifier(n_estimators = 200)
#LRI = LogisticRegression( )
#clf = AdaBoostClassifier(n_estimators = 200,base_estimator=LRI)
clf1.fit(spam_training_data,spam_training_target)


#spam_test_target_predict=clf.predict(spam_test_data)
#confusion_matrix(spam_test_target,spam_test_target_predict)
#classification_report(spam_test_target,spam_test_target_predict)
accuracy_score(spam_test_target,spam_test_target_predict)

#spam_test_target_predict=clf.predict(spam_test_data)
c_m = confusion_matrix(spam_test_target,spam_test_target_predict)
c_r = classification_report(spam_test_target,spam_test_target_predict)
a_s = accuracy_score(spam_test_target,spam_test_target_predict)


# Compare observed value and Predicted value
print("Prediction for 10 observation:    ",clf.predict(spam_test_data[0:20]))
print("Actual values for 10 observation: ",spam_test_target[0:20].values)
print(c_m)
print(c_r)
print(a_s)

# Commented out IPython magic to ensure Python compatibility.
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import ConfusionMatrixDisplay
# data visualization
import seaborn as sns
# %matplotlib inline
from matplotlib import pyplot as plt
from matplotlib import style
from sklearn.metrics import confusion_matrix

n_estimators = [10, 50, 100, 500, 1000, 5000]
max_features = ["sqrt", "log2", None]
highest_accuracy = 0
a = 0
b = ""

for x in max_features:
  for n in n_estimators:
    clf = RandomForestClassifier(n_estimators = n, max_features = x, random_state = 101)
    clf.fit(spam_training_data,spam_training_target)
    spam_test_target_predict=clf.predict(spam_test_data)
    print("For no_estimators = ",n,", and max_fetures = ", x , "the accuracy score is: ", accuracy_score(spam_test_target,spam_test_target_predict))

    if accuracy_score(spam_test_target,spam_test_target_predict) > highest_accuracy:
      highest_accuracy = accuracy_score(spam_test_target,spam_test_target_predict)
      a = n
      b = x

print("The random forest with the highest accuracy ",highest_accuracy, "has the following parameters: n_estimators = ", a, " and max_features = ", b)
clf = RandomForestClassifier(n_estimators = a, max_features = b, random_state = 101)
#clf = DecisionTreeClassifier(criterion = "entropy")
clf.fit(spam_training_data,spam_training_target)


spam_test_target_predict=clf.predict(spam_test_data)
c_m = confusion_matrix(spam_test_target,spam_test_target_predict)
c_r = classification_report(spam_test_target,spam_test_target_predict)
a_s = accuracy_score(spam_test_target,spam_test_target_predict)


# Compare observed value and Predicted value
print("Prediction for 10 observation:    ",clf.predict(spam_test_data[0:20]))
print("Actual values for 10 observation: ",spam_test_target[0:20].values)
print(c_m)
print(c_r)
print(a_s)

plt.figure(figsize=(10,8))
sns.heatmap(c_m,annot=True,fmt='d')